# ğŸ“˜ Deep Q-Network (DQN) for Atari Pong  
**Course:** CSCN8020 â€“ Reinforcement Learning Programming  
**Student:** *Krishna Reddy Bovilla*  
**Student ID:** *9050861*

---

## ğŸŒ Project Repository
GitHub: **https://github.com/bkrishnareddy-ai/Reinforcement-Learning-A3.git**

---

## ğŸ“ Project Description

This project implements a **Deep Q-Network (DQN)** agent capable of learning to play **Atari Pong** from **raw pixel input** using reinforcement learning.  
The implementation faithfully follows the original DeepMind DQN design, including:

- Experience Replay  
- Target Network  
- Convolutional Q-Network  
- Frame Preprocessing (Grayscale â†’ Resize â†’ Normalize)  
- Frame Stacking for temporal representation  
- Îµ-Greedy Exploration Strategy  

The project also includes **three controlled hyperparameter experiments** and a **full training analysis**, complete with plots and commentary.

---

# ğŸ› ï¸ Installation & Environment Setup

## 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/bkrishnareddy-ai/Reinforcement-Learning-A3.git
cd Reinforcement-Learning-A3
```

---

## 2ï¸âƒ£ Create a Python Environment

### âœ” Using Conda
```bash
conda create -n dqn-pong python=3.10 -y
conda activate dqn-pong
```

### âœ” Using venv
```bash
python -m venv dqn-pong
source dqn-pong/bin/activate   # Mac/Linux
dqn-pong\Scripts\activate    # Windows
```

---

## 3ï¸âƒ£ Install Dependencies

### Required Python Packages
```bash
pip install -q gym==0.26.2
pip install -q ale-py==0.7.5
pip install -q "autorom[accept-rom-license]"
AutoROM --accept-license
pip install torch numpy matplotlib tqdm
```

After installation, **AutoROM must be run** to download and install Atari ROMs.

---

# â–¶ï¸ Running the Notebook

Launch Jupyter and open the main notebook:

```bash
jupyter notebook
```

Open:

```
assignment3.ipynb
```

---

# ğŸ“ Project Structure

```
Reinforcement-Learning-A3/
â”‚
â”œâ”€â”€ assignment3_utils.py          # Frame preprocessing functions
â”œâ”€â”€ assignment3.ipynb    # Main notebook with full pipeline
â”œâ”€â”€ requirements.txt                    # Required libraries
â”œâ”€â”€ README.md                     # Documentation
â””â”€â”€ report.pdf                    # Final assignment report (optional)
```

---

# ğŸ§  Model Architecture (DeepMind DQN)

### Input  
- 4 stacked grayscale frames (84 Ã— 80)

### Convolutional Feature Extractor  
| Layer | Filters | Kernel | Stride | Activation |
|-------|----------|-----------|------------|--------------|
| Conv1 | 32 | 8Ã—8 | 4 | ReLU |
| Conv2 | 64 | 4Ã—4 | 2 | ReLU |
| Conv3 | 64 | 3Ã—3 | 1 | ReLU |

### Fully Connected  
- FC1: 512 units (ReLU)  
- Output: 6 Q-values representing Pong actions  

This architecture mirrors the original Deep Q-Network used by DeepMind.

---

# ğŸ§ª Experimental Configurations

Three training configurations were tested for 100 episodes each:

| Experiment | Batch Size | Target Update |
|------------|-------------|----------------|
| **Baseline** | 8 | Every 10 episodes |
| **Batch Size 16** | 16 | Every 10 episodes |
| **Target Update 3** | 8 | Every 3 episodes |

---

# ğŸ“Š Visualization & Analysis

The notebook generates:

### â¤ Individual Plots
- Score per episode  
- Moving average of last 5 episodes  

### â¤ Combined Comparison Plots
- Baseline vs. Batch 16 vs. Target Update 3 (score)
- Baseline vs. Batch 16 vs. Target Update 3 (moving average)

These help illustrate how hyperparameters affect learning.

---

# ğŸ Summary of Findings

### ğŸ”¹ Best Configuration  
âœ” **Batch size: 8**  
âœ” **Target update: 10**  

### ğŸ”¹ Why It Works Best
- Most stable reward behaviour  
- Preserves gradient sensitivity  
- Avoids oversmoothing from large batches  
- Avoids instability from overly frequent target updates  

### ğŸ”¹ Other Findings
- **Batch size 16** slowed learning  
- **Target update 3** caused unnecessary oscillations  

---


# ğŸ™Œ Acknowledgements

This project was developed for **CSCN8020 â€“ Reinforcement Learning Programming**,  
Mohawk College.

---

# ğŸ‰ End of README

